<div align="center">

# ðŸŒŒ Project Aura  
### **Real-Time Emotion Detection From Video Using YOLO**

<img src="https://img.shields.io/badge/Python-3.9+-blue?style=for-the-badge">
<img src="https://img.shields.io/badge/OpenCV-Enabled-green?style=for-the-badge">
<img src="https://img.shields.io/badge/YOLO-Emotion_Model-orange?style=for-the-badge">
<br>

<p style="font-size: 1.2rem; max-width: 700px; text-align: center;">
Project Aura processes video streams in real time, detects human faces, 
classifies emotional states, and overlays predictions directly onto video frames.
A flexible, modular pipeline built for research, experimentation, and development.
</p>

</div>

---

## ðŸŒŸ Features

-  **Face Detection** powered by a YOLOv8 face model  
-  **Emotion Classification** using a trained YOLO emotion model  
-  **Real-Time Annotation** with bounding boxes + emotion labels  
-  **Modular Architecture** with separate detection/classification modules  
-  **Optional Frame Extraction** for dataset creation and analysis  

---

##  Project Structure

```plaintext
Project Aura/
â”‚
â”œâ”€â”€ detect_faces.py           # Real-time face & emotion pipeline
â”œâ”€â”€ classify_emotions.py      # Emotion classification module
â”œâ”€â”€ main.py                   # Frame extraction utility
â”‚
â”œâ”€â”€ model.pt                  # Emotion model weights (optional)
â”œâ”€â”€ yolov8n-face.pt           # Face detection model weights (optional)
â”‚
â”œâ”€â”€ face_crops/               # Auto-generated (ignored)
â”œâ”€â”€ frames/                   # Auto-generated (ignored)
â”‚
â””â”€â”€ README.md
